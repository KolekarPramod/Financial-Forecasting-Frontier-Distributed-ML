{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KolekarPramod/Financial-Forecasting-Frontier-Distributed-ML/blob/main/Financial_Forecasting_Frontier_Distributed_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    Financial Forecasting Frontier Distributed ML\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -** Pramod Kolekar\n",
        "##### **Team Member 2 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Overview\n",
        "In the modern banking sector, the ability to efficiently process, analyze, and draw insights from vast volumes of data is crucial. Banks and financial institutions generate and collect extensive data, including customer demographics, transaction histories, market trends, and more. This data, when effectively analyzed, can lead to improved customer service, risk management, marketing strategies, and overall operational efficiency.\n",
        "\n",
        "###Project Background\n",
        "The banking industry faces challenges in managing and utilizing large datasets due to the volume, variety, and velocity of data. Traditional data processing methods often fall short in providing timely insights and handling real-time data streams. With the advent of distributed computing and machine learning technologies, banks now have the opportunity to harness these large datasets to make informed decisions, predict market trends, and enhance customer experiences."
      ],
      "metadata": {
        "id": "kTUQcS8c9uuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/KolekarPramod/Financial-Forecasting-Frontier-Distributed-ML"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Banking institutions generate massive volumes of data daily, which traditional computing systems struggle to process efficiently. To harness the full potential of this data, there is a need for scalable, distributed machine learning approaches that can store, process, and analyze large datasets in real time. This project focuses on leveraging distributed computing techniques to analyze the \"bank.csv\" dataset, simulating a real-world banking environment. The objective is to uncover customer behavior patterns, identify significant trends, and support data-driven decision-making through predictive analytics. The challenge lies in integrating data storage, querying, and machine learning within a distributed framework to produce actionable business insights.\n"
      ],
      "metadata": {
        "id": "8GufFheX-AOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, countDistinct,avg\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup Spark\n",
        "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()"
      ],
      "metadata": {
        "id": "-2tLRG0zsN5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load into PySpark\n",
        "df = spark.read.csv('bank_data.csv', header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "y6UOVgu7sHKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.columns"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.groupBy(df.columns).count().filter(\"count > 1\").count()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe().show()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\color{yellow}{\\text{age}}$ – Age of the individual (integer)\n",
        "\n",
        "$\\color{yellow}{\\text{job}}$ – Job type (string)\n",
        "\n",
        "$\\color{yellow}{\\text{marital}}$ – Marital status (string)\n",
        "\n",
        "$\\color{yellow}{\\text{education}}$ – Education level (string)\n",
        "\n",
        "$\\color{yellow}{\\text{default}}$ – Indicates if the individual has credit in default (string)\n",
        "\n",
        "$\\color{yellow}{\\text{balance}}$ – Account balance (integer)\n",
        "\n",
        "$\\color{yellow}{\\text{housing}}$ – Indicates if the individual has a housing loan (string)\n",
        "\n",
        "$\\color{yellow}{\\text{loan}}$ – Indicates if the individual has a personal loan (string)\n",
        "\n",
        "$\\color{yellow}{\\text{contact}}$ – Type of communication contact (string)\n",
        "\n",
        "$\\color{yellow}{\\text{day}}$ – Last contact day of the month (integer)\n",
        "\n",
        "$\\color{yellow}{\\text{month}}$ – Last contact month of the year (string)\n",
        "\n",
        "$\\color{yellow}{\\text{duration}}$ – Last contact duration, in seconds (integer)\n",
        "\n",
        "$\\color{yellow}{\\text{campaign}}$ – Number of contacts performed during this campaign for this client (integer)\n",
        "\n",
        "$\\color{yellow}{\\text{pdays}}$ – Number of days since the client was last contacted from a previous campaign (integer, -1 means not contacted)\n",
        "\n",
        "$\\color{yellow}{\\text{previous}}$ – Number of contacts before this campaign for the client (integer)\n",
        "\n",
        "$\\color{yellow}{\\text{poutcome}}$ – Outcome of the previous marketing campaign (string)\n",
        "\n",
        "$\\color{yellow}{\\text{y}}$ – Indicates if the client subscribed to a term deposit (string)"
      ],
      "metadata": {
        "id": "jwtgJ99swpii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.select([countDistinct(c).alias(c) for c in df.columns]).show()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"EDA with PySpark\").getOrCreate()"
      ],
      "metadata": {
        "id": "sgpk77g4yQXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get value counts from Spark\n",
        "vc_df = df.groupBy(\"y\").count().orderBy(\"count\", ascending=False)\n",
        "\n",
        "# Step 2: Convert to pandas (small data only)\n",
        "vc_pd = vc_df.toPandas()\n",
        "\n",
        "# Step 3: Plot bar chart\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(vc_pd[\"y\"], vc_pd[\"count\"], color='skyblue')\n",
        "plt.xlabel(\"Target: y (Subscribed)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Term Deposit Subscription Count (Yes/No)\")\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8ZF7OR9lzeqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that the majority of customers did **not** subscribe to the term deposit, indicating a strong class imbalance.\n",
        "Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Step 1: Get counts\n",
        "job_counts = df.groupBy(\"job\").count().orderBy(\"count\", ascending=False)\n",
        "\n",
        "# Step 2: Convert to pandas\n",
        "job_pd = job_counts.toPandas()\n",
        "\n",
        "# Step 3: Plot bar chart\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(job_pd[\"job\"], job_pd[\"count\"], color='orange')\n",
        "plt.xlabel(\"Job Type\")\n",
        "plt.ylabel(\"Number of Clients\")\n",
        "plt.title(\"Count of Clients by Job Type\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most clients are from management, blue-collar, and technician job types, while student and unknown categories have the fewest clients.\n",
        "Answer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Step 1: Group and count by education + target\n",
        "edu_counts = df.groupBy(\"education\", \"y\").count().orderBy(\"education\", \"y\")\n",
        "\n",
        "# Step 2: Pivot to wide format\n",
        "edu_pivot = edu_counts.groupBy(\"education\").pivot(\"y\").sum(\"count\").fillna(0)\n",
        "\n",
        "# Step 3: Convert to pandas and plot\n",
        "edu_pd = edu_pivot.toPandas()\n",
        "edu_pd[\"total\"] = edu_pd[\"yes\"] + edu_pd[\"no\"]\n",
        "edu_pd[\"subscription_rate\"] = edu_pd[\"yes\"] / edu_pd[\"total\"]\n",
        "\n",
        "# Step 4: Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(edu_pd[\"education\"], edu_pd[\"subscription_rate\"], color=\"green\")\n",
        "plt.xlabel(\"Education Level\")\n",
        "plt.ylabel(\"Subscription Rate\")\n",
        "plt.title(\"Subscription Rate by Education Level\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clients with a tertiary education have the highest subscription rate, while those with primary education have the lowest.\n",
        "Answer Here"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Group and count by housing status and y\n",
        "house_counts = df.groupBy(\"housing\", \"y\").count().orderBy(\"housing\", \"y\")\n",
        "\n",
        "# Pivot\n",
        "house_pivot = house_counts.groupBy(\"housing\").pivot(\"y\").sum(\"count\").fillna(0)\n",
        "\n",
        "# To pandas\n",
        "house_pd = house_pivot.toPandas()\n",
        "house_pd[\"total\"] = house_pd[\"yes\"] + house_pd[\"no\"]\n",
        "house_pd[\"subscription_rate\"] = house_pd[\"yes\"] / house_pd[\"total\"]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(house_pd[\"housing\"], house_pd[\"subscription_rate\"], color=\"purple\")\n",
        "plt.xlabel(\"Housing Loan\")\n",
        "plt.ylabel(\"Subscription Rate\")\n",
        "plt.title(\"Subscription Rate by Housing Loan\")\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clients without a housing loan have a significantly higher subscription rate than those with a housing loan.\n",
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Compute average balance by 'y'\n",
        "avg_balance = df.groupBy(\"y\").agg(avg(\"balance\").alias(\"avg_balance\")).orderBy(\"y\")\n",
        "\n",
        "# To pandas\n",
        "avg_balance_pd = avg_balance.toPandas()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(avg_balance_pd[\"y\"], avg_balance_pd[\"avg_balance\"], color=\"teal\")\n",
        "plt.xlabel(\"Subscription Status\")\n",
        "plt.ylabel(\"Average Account Balance\")\n",
        "plt.title(\"Average Balance vs Subscription\")\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clients who subscribed to the term deposit tend to have higher average account balances compared to those who did not.\n",
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BankTermDepositPrediction\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "fLBRtm07ijcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Loads a banking dataset and infers schema automatically.\n",
        "2. Converts categorical columns and target variable into numerical indices using `StringIndexer`.\n",
        "3. Combines numerical and indexed categorical features into a single feature vector.\n",
        "4. Builds and trains a Random Forest classifier within a PySpark `Pipeline`, then evaluates it using AUC.\n",
        "5. Saves the trained model and stops the Spark session.\n"
      ],
      "metadata": {
        "id": "nDJQOl_zXZ2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data_path = \"/content/bank_data.csv\"\n",
        "data = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "\n",
        "# Handle categorical variables using StringIndexer\n",
        "categorical_columns = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_columns]\n",
        "\n",
        "# Rename the target column to 'label'\n",
        "data = data.withColumnRenamed(\"y\", \"label\")\n",
        "\n",
        "# Convert the label column to numerical values\n",
        "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "data = label_indexer.fit(data).transform(data)\n",
        "\n",
        "# Assemble features into a single vector\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"] + [col+\"_index\" for col in categorical_columns],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf = RandomForestClassifier(labelCol=\"label_index\", featuresCol=\"features\", numTrees=100)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline(stages=indexers + [assembler, rf])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.transform(test_data)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label_index\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"Test AUC: {auc}\")\n",
        "\n",
        "\n",
        "# Save the trained model, overwriting if the path already exists\n",
        "model.write().overwrite().save(\"/content/ml_model_trained\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "yVFQz1Th6OX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Real-Time Transaction Analysis:***"
      ],
      "metadata": {
        "id": "gG_bZ4MlM-7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the original CSV\n",
        "df = pd.read_csv(\"/content/bank_data.csv\")\n",
        "\n",
        "# Add synthetic timestamps\n",
        "from datetime import datetime, timedelta\n",
        "base_time = datetime.now()\n",
        "df[\"event_time\"] = [base_time + timedelta(seconds=i) for i in range(len(df))]\n",
        "\n",
        "# Create input directory for streaming\n",
        "stream_dir = \"/content/stream_input\"\n",
        "os.makedirs(stream_dir, exist_ok=True)\n",
        "\n",
        "# Save each row as a separate CSV file\n",
        "for i, row in df.iterrows():\n",
        "\n",
        "    row_df = pd.DataFrame([row])\n",
        "    row_df.to_csv(f\"{stream_dir}/txn_{i:05d}.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "vXHa_RlCROfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Window Operations and Trend Analysis:**\n",
        "\n",
        "\n",
        "\n",
        "1. **Starts a Spark Structured Streaming Session**:\n",
        "   Initializes a SparkSession configured to run locally with all CPU cores, naming the application \"StreamingBankAnalysis\".\n",
        "\n",
        "2. **Defines a Schema for Incoming CSV Data**:\n",
        "   Sets a structured schema for a dataset that includes customer demographic and banking details (like age, job, balance, etc.), including a TimestampType field called event_time to enable time-based operations.\n",
        "\n",
        "3. **Reads Streaming CSV Data**:\n",
        "   Continuously reads CSV files from the `/content/stream_input` directory as a streaming DataFrame using the predefined schema and with headers enabled.\n",
        "\n",
        "4. **Performs Time-Based Aggregation**:\n",
        "   Groups the incoming streaming data by a 10-second time window and the job field, then counts how many records fall into each (window, job) combination.\n",
        "\n",
        "5. **Outputs Results to Console in Real-Time**:\n",
        "   Writes the aggregated result to the console in *complete* output mode, meaning the full result table is printed every time it's updated. It continues running indefinitely until manually stopped.\n",
        "\n"
      ],
      "metadata": {
        "id": "xcRPm5BHSh9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, window\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamingBankAnalysis\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField(\"age\", IntegerType()),\n",
        "    StructField(\"job\", StringType()),\n",
        "    StructField(\"marital\", StringType()),\n",
        "    StructField(\"education\", StringType()),\n",
        "    StructField(\"default\", StringType()),\n",
        "    StructField(\"balance\", IntegerType()),\n",
        "    StructField(\"housing\", StringType()),\n",
        "    StructField(\"loan\", StringType()),\n",
        "    StructField(\"contact\", StringType()),\n",
        "    StructField(\"day\", IntegerType()),\n",
        "    StructField(\"month\", StringType()),\n",
        "    StructField(\"duration\", IntegerType()),\n",
        "    StructField(\"campaign\", IntegerType()),\n",
        "    StructField(\"pdays\", IntegerType()),\n",
        "    StructField(\"previous\", IntegerType()),\n",
        "    StructField(\"poutcome\", StringType()),\n",
        "    StructField(\"y\", StringType()),\n",
        "    StructField(\"event_time\", TimestampType()),\n",
        "])\n",
        "\n",
        "# Stream read from /content/stream_input\n",
        "df_stream = spark.readStream \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"header\", True) \\\n",
        "    .csv(\"/content/stream_input\")\n",
        "\n",
        "# Simple aggregation: count by job per 10-second window\n",
        "agg = df_stream.groupBy(\n",
        "    window(\"event_time\", \"10 seconds\"),\n",
        "    \"job\"\n",
        ").count()\n",
        "\n",
        "query = agg.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .option(\"truncate\", False) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()\n"
      ],
      "metadata": {
        "id": "8TrkLUYbRSbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Efficient Data Handling through Data Parallelism***"
      ],
      "metadata": {
        "id": "x_IFXhMNaSNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BankingDataParallelism\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load CSV data in parallel\n",
        "df = spark.read.csv(\"/content/bank_data.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "jnu4Yr7JaIlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel Grouping and Aggregation"
      ],
      "metadata": {
        "id": "0TWe6sVxcHZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parallel computation of average balance by job type\n",
        "df.groupBy(\"job\") \\\n",
        "  .avg(\"balance\") \\\n",
        "  .show()\n"
      ],
      "metadata": {
        "id": "nYVwTNDKaU5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Transformation in Parallel"
      ],
      "metadata": {
        "id": "0tXd895VcLF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new feature in parallel\n",
        "df = df.withColumn(\"balance_k\", df[\"balance\"] / 1000)\n"
      ],
      "metadata": {
        "id": "xp_vf_CXcM-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel Filtering + Export"
      ],
      "metadata": {
        "id": "47TlZCACcPU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartition to utilize more cores/nodes\n",
        "df = df.repartition(8)  # or spark.sparkContext.defaultParallelism\n"
      ],
      "metadata": {
        "id": "5_e1UzjOcP-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "QiSjWBuncUSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}